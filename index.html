<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant Console</title>
    <link rel="stylesheet" href="voice-chat.css">
    <style>
        .description-box {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            margin: 10px 0;
            height: 150px;
            max-height: 150px;
            font-size: 14px;
            line-height: 1.6;
            color: #333;
            overflow-y: auto;
            overflow-x: hidden;
            white-space: pre-wrap;
            word-wrap: break-word;
            scrollbar-width: thin;
            scrollbar-color: #888 #f5f5f5;
        }

        .description-box::-webkit-scrollbar {
            width: 8px;
        }

        .description-box::-webkit-scrollbar-track {
            background: #f5f5f5;
            border-radius: 4px;
        }

        .description-box::-webkit-scrollbar-thumb {
            background-color: #888;
            border-radius: 4px;
            border: 2px solid #f5f5f5;
        }

        .description-box::-webkit-scrollbar-thumb:hover {
            background-color: #666;
        }
    </style>
</head>
<body>
    <div class="main-container">
        <div class="dev-console">
            <div class="console-container">
                <div class="console-header">
                    <span>Development Console</span>
                    <button id="clearConsole">Clear</button>
                </div>
                <div class="console" id="console"></div>
            </div>
            
            <div class="console-footer">
                <div class="status-group">
                    <div class="status-indicator" id="status">
                        <div class="status-dot"></div>
                        <span class="status-text">Ready</span>
                    </div>
                </div>
                
                <div class="controls-group">
                    <div class="response-indicator" id="responseIndicator">
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                    </div>
                    
                    <button class="voice-button" id="voiceButton">
                        <div class="mic-icon"></div>
                    </button>

                    <div class="camera-controls">
                        <select id="micSelect" class="device-select">
                            <option value="">Select microphone...</option>
                        </select>
                        <select id="cameraSelect" class="device-select">
                            <option value="">Select camera...</option>
                        </select>
                        <button id="captureButton" class="capture-button">
                            Capture
                        </button>
                    </div>
                </div>
            </div>
        </div>

        <div class="camera-section">
            <div class="camera-preview">
                <video id="cameraFeed" autoplay playsinline></video>
            </div>
            <div class="description-box" id="descriptionBox">
                Heres gonna be the text description of the image
            </div>
        </div>
    </div>

    <script>
        let ws;
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;
        let audioContext;
        let audioQueue = [];
        let isPlaying = false;
        let lastPlayTime = 0;
        let currentAudioStream = null;
        const ORIGINAL_SAMPLE_RATE = 44100;
        const PLAYBACK_SAMPLE_RATE = 22050;
        const PLAYBACK_RATE = 0.5;

        const voiceButton = document.getElementById('voiceButton');
        const statusElement = document.getElementById('status');
        const responseIndicator = document.getElementById('responseIndicator');
        const statusDot = document.querySelector('.status-dot');
        const statusText = document.querySelector('.status-text');
        const consoleElement = document.getElementById('console');
        const clearConsoleButton = document.getElementById('clearConsole');
        const micSelect = document.getElementById('micSelect');

        // Add new camera functionality
        const cameraFeed = document.getElementById('cameraFeed');
        const cameraSelect = document.getElementById('cameraSelect');
        const captureButton = document.getElementById('captureButton');
        let currentStream = null;

        function appendToConsole(message, type = 'info') {
            const entry = document.createElement('div');
            entry.classList.add('console-entry', type);
            
            // Try to parse JSON if the message is a string
            if (typeof message === 'string') {
                try {
                    const jsonObj = JSON.parse(message);
                    message = JSON.stringify(jsonObj, null, 2);
                } catch (e) {
                    // If not JSON, use as is
                }
            }

            entry.textContent = typeof message === 'object' ? 
                JSON.stringify(message, null, 2) : message;
            
            consoleElement.appendChild(entry);
            consoleElement.scrollTop = consoleElement.scrollHeight;
        }

        clearConsoleButton.addEventListener('click', () => {
            consoleElement.innerHTML = '';
        });

        function connect() {
            ws = new WebSocket('ws://localhost:8000/conversations/realtime');

            ws.onopen = () => {
                statusDot.classList.add('connected');
                statusText.textContent = 'Ready';
                appendToConsole('WebSocket connected', 'success');
            };

            ws.onclose = () => {
                statusDot.classList.remove('connected');
                statusText.textContent = 'Disconnected';
                appendToConsole('WebSocket disconnected', 'error');
                setTimeout(connect, 1000);
            };

            ws.onerror = () => {
                statusDot.classList.remove('connected');
                statusText.textContent = 'Error';
                appendToConsole('WebSocket error', 'error');
            };

            ws.onmessage = async (event) => {
                if (event.data instanceof Blob) {
                    appendToConsole('Received audio chunk', 'info');
                    // Convert blob to array buffer
                    const arrayBuffer = await event.data.arrayBuffer();
                    // Convert to Int16Array since we're receiving PCM 16-bit data
                    const pcmData = new Int16Array(arrayBuffer);
                    
                    // Convert PCM 16-bit to Float32 for Web Audio API
                    const floatData = new Float32Array(pcmData.length);
                    for (let i = 0; i < pcmData.length; i++) {
                        // Convert 16-bit integer to float32 (-1.0 to 1.0)
                        floatData[i] = pcmData[i] / 32768.0;
                    }
                    
                    // Resample the audio data to slow it down
                    const resampledData = new Float32Array(Math.floor(floatData.length * (PLAYBACK_SAMPLE_RATE / ORIGINAL_SAMPLE_RATE)));
                    for (let i = 0; i < resampledData.length; i++) {
                        const originalIndex = Math.floor(i * (ORIGINAL_SAMPLE_RATE / PLAYBACK_SAMPLE_RATE));
                        resampledData[i] = floatData[originalIndex];
                    }
                    
                    audioQueue.push(resampledData);
                    
                    // If not currently playing, start playing
                    if (!isPlaying) {
                        playNextChunk();
                    }
                } else {
                    // Handle text message
                    try {
                        const data = event.data;
                        appendToConsole(`Received: ${data}`, 'received');
                        
                        // Try to parse as JSON if it's a JSON string
                        try {
                            const jsonData = JSON.parse(data);
                            appendToConsole('Parsed JSON:', 'info');
                            appendToConsole(jsonData, 'received');
                        } catch (e) {
                            // If it's not JSON, we already logged the raw text
                        }
                    } catch (e) {
                        appendToConsole(`Error processing message: ${e.message}`, 'error');
                    }
                }
            };
        }

        async function playNextChunk() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                responseIndicator.classList.remove('active');
                return;
            }

            isPlaying = true;
            responseIndicator.classList.add('active');

            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: PLAYBACK_SAMPLE_RATE
                });
            }

            const floatData = audioQueue.shift();
            const currentTime = audioContext.currentTime;
            
            // Create an audio buffer
            const audioBuffer = audioContext.createBuffer(1, floatData.length, PLAYBACK_SAMPLE_RATE);
            const channelData = audioBuffer.getChannelData(0);
            channelData.set(floatData);

            // Create and configure source
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.playbackRate.value = PLAYBACK_RATE; // Set playback rate
            source.connect(audioContext.destination);
            
            // Calculate when this chunk should start playing
            const startTime = Math.max(currentTime, lastPlayTime);
            lastPlayTime = startTime + (audioBuffer.duration / PLAYBACK_RATE);
            
            // Schedule the next chunk
            source.onended = () => {
                if (audioQueue.length > 0) {
                    playNextChunk();
                } else {
                    isPlaying = false;
                    responseIndicator.classList.remove('active');
                }
            };

            try {
                source.start(startTime);
                appendToConsole('Playing audio chunk', 'info');
            } catch (error) {
                console.error('Error playing audio chunk:', error);
                appendToConsole('Error playing audio chunk: ' + error.message, 'error');
                // If there's an error, try the next chunk
                lastPlayTime = currentTime;
                playNextChunk();
            }
        }

        // Add event listener for microphone selection
        micSelect.addEventListener('change', async () => {
            if (currentAudioStream) {
                currentAudioStream.getTracks().forEach(track => track.stop());
            }
            if (micSelect.value) {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: { deviceId: { exact: micSelect.value } }
                    });
                    currentAudioStream = stream;
                    appendToConsole('Switched to microphone: ' + micSelect.selectedOptions[0].text, 'info');
                } catch (error) {
                    appendToConsole('Error switching microphone: ' + error.message, 'error');
                }
            }
        });

        // Function to enumerate audio devices
        async function setupAudioDevices() {
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioDevices = devices.filter(device => device.kind === 'audioinput');
                
                micSelect.innerHTML = '';
                let defaultDevice = null;

                audioDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Microphone ${index + 1}`;
                    micSelect.appendChild(option);
                    
                    // Check if this is the default device
                    if (device.deviceId === 'default') {
                        defaultDevice = device;
                    }
                });

                // If we found a default device, select it
                if (defaultDevice) {
                    micSelect.value = defaultDevice.deviceId;
                    // Trigger the change event to set up the stream
                    const event = new Event('change');
                    micSelect.dispatchEvent(event);
                } else if (audioDevices.length > 0) {
                    // If no default device, select the first one
                    micSelect.value = audioDevices[0].deviceId;
                    const event = new Event('change');
                    micSelect.dispatchEvent(event);
                }

                appendToConsole('Found ' + audioDevices.length + ' audio input devices', 'info');
            } catch (error) {
                appendToConsole('Error enumerating audio devices: ' + error.message, 'error');
            }
        }

        // Function to switch camera
        async function switchCamera(deviceId = null) {
            if (currentStream) {
                currentStream.getTracks().forEach(track => track.stop());
            }

            try {
                const constraints = {
                    video: deviceId ? 
                        { deviceId: { exact: deviceId } } : 
                        true
                };

                appendToConsole('Requesting camera with constraints: ' + JSON.stringify(constraints), 'info');
                
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                currentStream = stream;
                cameraFeed.srcObject = stream;
                
                // Ensure video is playing
                try {
                    await cameraFeed.play();
                    appendToConsole('Camera feed started successfully', 'success');
                } catch (playError) {
                    appendToConsole('Error playing video: ' + playError.message, 'error');
                }

                // Log active track settings
                const videoTrack = stream.getVideoTracks()[0];
                appendToConsole('Active video track settings: ' + JSON.stringify(videoTrack.getSettings()), 'info');
                
            } catch (error) {
                appendToConsole('Error accessing camera: ' + error.message, 'error');
            }
        }

        // Populate camera select with available devices
        async function setupCameras() {
            try {
                // First start with default camera
                await switchCamera();
                appendToConsole('Default camera initialized', 'success');

                // Then enumerate devices
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');
                
                appendToConsole('Found ' + videoDevices.length + ' video devices', 'info');
                
                cameraSelect.innerHTML = '';
                videoDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Camera ${index + 1}`;
                    cameraSelect.appendChild(option);
                    appendToConsole('Added camera option: ' + option.text, 'info');
                });

                // Select the current camera in the dropdown
                if (currentStream) {
                    const currentTrack = currentStream.getVideoTracks()[0];
                    if (currentTrack) {
                        const currentDevice = videoDevices.find(device => device.deviceId === currentTrack.getSettings().deviceId);
                        if (currentDevice) {
                            cameraSelect.value = currentDevice.deviceId;
                        }
                    }
                }

            } catch (error) {
                appendToConsole('Error setting up cameras: ' + error.message, 'error');
            }
        }

        // Handle camera selection
        cameraSelect.addEventListener('change', () => {
            if (cameraSelect.value) {
                appendToConsole('Switching to camera: ' + cameraSelect.selectedOptions[0].text, 'info');
                switchCamera(cameraSelect.value);
            }
        });

        // Handle capture button
        captureButton.addEventListener('click', async () => {
            try {
                // Create a canvas element to capture the image
                const canvas = document.createElement('canvas');
                const context = canvas.getContext('2d');
                
                // Set canvas size to match video dimensions
                canvas.width = cameraFeed.videoWidth;
                canvas.height = cameraFeed.videoHeight;
                
                // Draw the current video frame to the canvas
                context.drawImage(cameraFeed, 0, 0, canvas.width, canvas.height);
                
                // Convert the canvas to a blob
                const blob = await new Promise(resolve => {
                    canvas.toBlob(resolve, 'image/jpeg', 0.95);
                });
                
                // Create form data and append the image
                const formData = new FormData();
                formData.append('image', blob, 'capture.jpg');
                
                appendToConsole('Sending image to server...', 'info');
                
                // Update description box with loading message
                const descriptionBox = document.getElementById('descriptionBox');
                descriptionBox.textContent = 'Waiting for message description...';
                
                // Send the image to the server
                const response = await fetch('http://localhost:8000/conversations/image/', {
                    method: 'POST',
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                
                const data = await response.json();
                appendToConsole('Server response:', 'received');
                appendToConsole(data, 'received');
                
                // Update the description box with the transcription
                descriptionBox.textContent = data.transcription;
                
            } catch (error) {
                appendToConsole('Error capturing/sending image: ' + error.message, 'error');
                // Update description box with error message
                const descriptionBox = document.getElementById('descriptionBox');
                descriptionBox.textContent = 'Error: Failed to get image description';
            }
        });


        async function setupAudioRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                // Create an AudioContext for processing
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                }

                const TARGET_SAMPLE_RATE = 24000;
                const ORIGINAL_SAMPLE_RATE = audioContext.sampleRate;
                const DOWNSAMPLE_FACTOR = Math.floor(ORIGINAL_SAMPLE_RATE / TARGET_SAMPLE_RATE);
                
                // Create a MediaStreamSource from the stream
                const source = audioContext.createMediaStreamSource(stream);
                
                // Create a ScriptProcessorNode for raw audio processing
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                let recordedChunks = [];
                let sampleCounter = 0;
                let accumulator = 0;
                
                processor.onaudioprocess = (e) => {
                    if (isRecording) {
                        // Get the raw audio data
                        const inputData = e.inputBuffer.getChannelData(0);
                        
                        // Downsample to 24kHz by averaging samples
                        const downsampledLength = Math.floor(inputData.length / DOWNSAMPLE_FACTOR);
                        const downsampledData = new Float32Array(downsampledLength);
                        
                        for (let i = 0; i < inputData.length; i++) {
                            accumulator += inputData[i];
                            sampleCounter++;
                            
                            if (sampleCounter === DOWNSAMPLE_FACTOR) {
                                downsampledData[Math.floor(i / DOWNSAMPLE_FACTOR)] = accumulator / DOWNSAMPLE_FACTOR;
                                accumulator = 0;
                                sampleCounter = 0;
                            }
                        }
                        
                        // Convert Float32Array to Int16Array (PCM 16-bit)
                        const pcmData = new Int16Array(downsampledData.length);
                        for (let i = 0; i < downsampledData.length; i++) {
                            // Convert float to 16-bit signed integer
                            const s = Math.max(-1, Math.min(1, downsampledData[i]));
                            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }
                        
                        recordedChunks.push(pcmData.buffer);
                    }
                };

                voiceButton.addEventListener('click', () => {
                    if (!isRecording) {
                        // Start recording
                        recordedChunks = [];
                        sampleCounter = 0;
                        accumulator = 0;
                        isRecording = true;
                        voiceButton.classList.add('recording');
                        statusText.textContent = 'Listening...';
                        appendToConsole('Started recording at 24kHz mono', 'info');
                    } else {
                        // Stop recording
                        isRecording = false;
                        voiceButton.classList.remove('recording');
                        statusText.textContent = 'Processing...';
                        
                        // Concatenate all chunks
                        const audioData = new Int16Array(
                            recordedChunks.reduce((acc, buffer) => acc + buffer.byteLength, 0)
                        );
                        
                        let offset = 0;
                        recordedChunks.forEach(buffer => {
                            audioData.set(new Int16Array(buffer), offset);
                            offset += buffer.byteLength / 2;
                        });
                        
                        // Send the PCM data
                        if (ws.readyState === WebSocket.OPEN) {
                            ws.send(audioData.buffer);
                            appendToConsole('Sent PCM audio data (24kHz, mono)', 'sent');
                        }
                        
                        recordedChunks = [];
                        appendToConsole('Stopped recording', 'info');
                    }
                });

                appendToConsole('Audio recording setup complete (24kHz, mono)', 'success');
            } catch (err) {
                console.error('Error accessing microphone:', err);
                statusText.textContent = 'Microphone Error';
                appendToConsole('Microphone Error: ' + err.message, 'error');
            }
        }


        // Initialize camera system
        async function initializeCamera() {
            try {
                // Check if getUserMedia is supported
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    throw new Error('Camera API not supported in this browser');
                }

                appendToConsole('Initializing camera system...', 'info');
                await setupCameras();

            } catch (error) {
                appendToConsole('Camera initialization error: ' + error.message, 'error');
            }
        }

        // Start camera initialization
        initializeCamera().catch(error => {
            appendToConsole('Fatal camera error: ' + error.message, 'error');
        });

        // Initial setup
        connect();
        setupAudioRecording();
    </script>
</body>
</html> 