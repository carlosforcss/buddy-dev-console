<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant Console</title>
    <link rel="stylesheet" href="voice-chat.css">
    <style>
        .description-box {
            background-color: #f5f5f5;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 15px;
            margin: 10px 0;
            height: 150px;
            max-height: 150px;
            font-size: 14px;
            line-height: 1.6;
            color: #333;
            overflow-y: auto;
            overflow-x: hidden;
            white-space: pre-wrap;
            word-wrap: break-word;
            scrollbar-width: thin;
            scrollbar-color: #888 #f5f5f5;
        }

        .description-box::-webkit-scrollbar {
            width: 8px;
        }

        .description-box::-webkit-scrollbar-track {
            background: #f5f5f5;
            border-radius: 4px;
        }

        .description-box::-webkit-scrollbar-thumb {
            background-color: #888;
            border-radius: 4px;
            border: 2px solid #f5f5f5;
        }

        .description-box::-webkit-scrollbar-thumb:hover {
            background-color: #666;
        }

        /* Environment Toggle Styles */
        .environment-toggle {
            display: flex;
            align-items: center;
            margin-right: 15px;
        }

        .toggle-label {
            margin-left: 8px;
            font-size: 14px;
            color: #666;
        }

        .switch {
            position: relative;
            display: inline-block;
            width: 50px;
            height: 24px;
        }

        .switch input {
            opacity: 0;
            width: 0;
            height: 0;
        }

        .slider {
            position: absolute;
            cursor: pointer;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background-color: #ccc;
            transition: .4s;
        }

        .slider:before {
            position: absolute;
            content: "";
            height: 16px;
            width: 16px;
            left: 4px;
            bottom: 4px;
            background-color: white;
            transition: .4s;
        }

        input:checked + .slider {
            background-color: #2196F3;
        }

        input:focus + .slider {
            box-shadow: 0 0 1px #2196F3;
        }

        input:checked + .slider:before {
            transform: translateX(26px);
        }

        .slider.round {
            border-radius: 24px;
        }

        .slider.round:before {
            border-radius: 50%;
        }

        .start-session-button {
            padding: 10px 20px;
            background-color: #4CAF50;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 14px;
            transition: all 0.3s ease;
            margin-right: 15px;
        }

        .start-session-button:hover {
            background-color: #45a049;
        }

        .start-session-button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
    </style>
</head>
<body>
    <div class="main-container">
        <div class="dev-console">
            <div class="console-container">
                <div class="console-header">
                    <span>Development Console</span>
                    <button id="clearConsole">Clear</button>
                </div>
                <div class="console" id="console"></div>
            </div>
            
            <div class="console-footer">
                <div class="status-group">
                    <div class="status-indicator" id="status">
                        <div class="status-dot"></div>
                        <span class="status-text">Ready</span>
                    </div>
                </div>
                
                <div class="controls-group">
                    <button id="startSessionButton" class="start-session-button">
                        Start Session
                    </button>

                    <div class="response-indicator" id="responseIndicator">
                        <div class="wave"></div>
                        <div class="wave"></div>
                        <div class="wave"></div>
                    </div>
                    
                    <button class="voice-button" id="voiceButton" style="display: none;">
                        <div class="mic-icon"></div>
                    </button>

                    <div class="environment-toggle">
                        <label class="switch">
                            <input type="checkbox" id="envToggle">
                            <span class="slider round"></span>
                        </label>
                        <span class="toggle-label">Production</span>
                    </div>

                    <div class="camera-controls">
                        <select id="micSelect" class="device-select">
                            <option value="">Select microphone...</option>
                        </select>
                        <select id="cameraSelect" class="device-select">
                            <option value="">Select camera...</option>
                        </select>
                        <button id="captureButton" class="capture-button">
                            Capture
                        </button>
                    </div>
                </div>
            </div>
        </div>

        <div class="camera-section">
            <div class="camera-preview">
                <video id="cameraFeed" autoplay playsinline></video>
            </div>
            <div class="description-box" id="descriptionBox">
                Heres gonna be the text description of the image
            </div>
        </div>
    </div>

    <script>
        let audioChunks = [];
        let isRecording = false;
        let audioContext;
        let audioQueue = [];
        let isPlaying = false;
        let lastPlayTime = 0;
        let currentAudioStream = null;
        let currentSession = null;
        let ws = null;
        const ORIGINAL_SAMPLE_RATE = 44100;
        const PLAYBACK_SAMPLE_RATE = 22050;
        const PLAYBACK_RATE = 0.5;

        const envConfig = {
            dev: {
                wsUrl: 'ws://localhost:8000/conversations/realtime',
                apiUrl: 'http://localhost:8000'
            },
            prod: {
                wsUrl: 'ws://localhost/conversations/realtime',
                apiUrl: 'http://localhost'
            }
        };
        let currentEnv = localStorage.getItem('currentEnv') || 'dev';

        const voiceButton = document.getElementById('voiceButton');
        const startSessionButton = document.getElementById('startSessionButton');
        const statusElement = document.getElementById('status');
        const responseIndicator = document.getElementById('responseIndicator');
        const statusDot = document.querySelector('.status-dot');
        const statusText = document.querySelector('.status-text');
        const consoleElement = document.getElementById('console');
        const clearConsoleButton = document.getElementById('clearConsole');
        const micSelect = document.getElementById('micSelect');
        const envToggle = document.getElementById('envToggle');

        // Initialize toggle state from localStorage
        envToggle.checked = currentEnv === 'prod';

        // Console logging functionality
        function appendToConsole(message, type = 'info') {
            const entry = document.createElement('div');
            entry.classList.add('console-entry', type);
            
            // Try to parse JSON if the message is a string
            if (typeof message === 'string') {
                try {
                    const jsonObj = JSON.parse(message);
                    message = JSON.stringify(jsonObj, null, 2);
                } catch (e) {
                    // If not JSON, use as is
                }
            }

            entry.textContent = typeof message === 'object' ? 
                JSON.stringify(message, null, 2) : message;
            
            consoleElement.appendChild(entry);
            consoleElement.scrollTop = consoleElement.scrollHeight;
        }

        clearConsoleButton.addEventListener('click', () => {
            consoleElement.innerHTML = '';
        });

        // Audio processing worklet code
        const workletCode = `
            class AudioProcessor extends AudioWorkletProcessor {
                constructor() {
                    super();
                    this.isRecording = false;
                    this.sampleCounter = 0;
                    this.accumulator = 0;
                    this.downsampleFactor = 2; // For 24kHz from 48kHz
                }

                process(inputs, outputs, parameters) {
                    const input = inputs[0];
                    if (!input || !input.length) return true;

                    const inputChannel = input[0];
                    if (this.isRecording) {
                        // Downsample to 24kHz
                        const downsampledData = new Float32Array(Math.floor(inputChannel.length / this.downsampleFactor));
                        
                        for (let i = 0; i < inputChannel.length; i++) {
                            this.accumulator += inputChannel[i];
                            this.sampleCounter++;
                            
                            if (this.sampleCounter === this.downsampleFactor) {
                                downsampledData[Math.floor(i / this.downsampleFactor)] = 
                                    this.accumulator / this.downsampleFactor;
                                this.accumulator = 0;
                                this.sampleCounter = 0;
                            }
                        }

                        // Convert to PCM 16-bit and send to main thread
                        const pcmData = new Int16Array(downsampledData.length);
                        for (let i = 0; i < downsampledData.length; i++) {
                            const s = Math.max(-1, Math.min(1, downsampledData[i]));
                            pcmData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }

                        this.port.postMessage(pcmData.buffer);
                    }
                    return true;
                }
            }

            registerProcessor('audio-processor', AudioProcessor);
        `;

        async function setupAudioRecording() {
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        channelCount: 1, // Force mono
                        sampleRate: 48000 // Request high sample rate for better downsampling
                    } 
                });
                
                // Create an AudioContext
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 48000 // Force 48kHz for consistent downsampling
                    });
                }

                // Create audio source
                const source = audioContext.createMediaStreamSource(stream);
                
                // Create a ScriptProcessorNode for raw audio processing
                const processor = audioContext.createScriptProcessor(4096, 1, 1);
                
                source.connect(processor);
                processor.connect(audioContext.destination);
                
                let recordedChunks = [];
                const TARGET_SAMPLE_RATE = 24000;
                const DOWNSAMPLE_FACTOR = audioContext.sampleRate / TARGET_SAMPLE_RATE;
                
                let sampleCounter = 0;
                let accumulator = 0;
                
                processor.onaudioprocess = (e) => {
                    if (isRecording) {
                        // Get mono channel data
                        const inputData = e.inputBuffer.getChannelData(0);
                        
                        // Calculate downsampled buffer size
                        const downsampledLength = Math.floor(inputData.length / DOWNSAMPLE_FACTOR);
                        const downsampledData = new Float32Array(downsampledLength);
                        
                        // Downsample to exactly 24kHz
                        for (let i = 0; i < downsampledLength; i++) {
                            const inputIndex = Math.floor(i * DOWNSAMPLE_FACTOR);
                            let sum = 0;
                            let count = 0;
                            
                            // Average samples for better quality
                            for (let j = 0; j < DOWNSAMPLE_FACTOR && (inputIndex + j) < inputData.length; j++) {
                                sum += inputData[inputIndex + j];
                                count++;
                            }
                            
                            downsampledData[i] = sum / count;
                        }
                        
                        // Convert to 16-bit PCM
                        const pcmData = new Int16Array(downsampledData.length);
                        for (let i = 0; i < downsampledData.length; i++) {
                            // Ensure the sample is between -1 and 1
                            const sample = Math.max(-1, Math.min(1, downsampledData[i]));
                            // Convert to 16-bit PCM
                            pcmData[i] = Math.round(sample * 32767);
                        }
                        
                        recordedChunks.push(pcmData.buffer);
                    }
                };

                voiceButton.addEventListener('click', () => {
                    if (!isRecording) {
                        // Start recording
                        recordedChunks = [];
                        isRecording = true;
                        voiceButton.classList.add('recording');
                        statusText.textContent = 'Listening...';
                        appendToConsole('Started recording at 24kHz mono', 'info');
                    } else {
                        // Stop recording
                        isRecording = false;
                        voiceButton.classList.remove('recording');
                        statusText.textContent = 'Processing...';
                        
                        // Calculate total length of all chunks
                        const totalLength = recordedChunks.reduce((acc, buffer) => acc + buffer.byteLength, 0);
                        const audioData = new Int16Array(totalLength / 2);
                        let offset = 0;
                        
                        // Combine all chunks
                        recordedChunks.forEach(buffer => {
                            const view = new Int16Array(buffer);
                            audioData.set(view, offset);
                            offset += view.length;
                        });
                        
                        // Send the PCM data
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            ws.send(audioData.buffer);
                            appendToConsole(`Sent PCM audio data (${audioData.length} samples at 24kHz mono)`, 'sent');
                        }
                        
                        recordedChunks = [];
                        appendToConsole('Stopped recording', 'info');
                    }
                });

                appendToConsole('Audio recording setup complete (24kHz mono PCM16)', 'success');
            } catch (err) {
                console.error('Error accessing microphone:', err);
                statusText.textContent = 'Microphone Error';
                appendToConsole('Microphone Error: ' + err.message, 'error');
            }
        }

        // Add camera functionality
        const cameraFeed = document.getElementById('cameraFeed');
        const cameraSelect = document.getElementById('cameraSelect');
        const captureButton = document.getElementById('captureButton');
        let currentStream = null;

        function connect() {
            if (!currentSession) {
                appendToConsole('Cannot connect WebSocket: No active session', 'error');
                return;
            }

            if (ws) {
                ws.close();
            }

            const wsUrl = `${envConfig[currentEnv].wsUrl}/${currentSession.id}`;
            ws = new WebSocket(wsUrl);

            ws.onopen = () => {
                statusDot.classList.add('connected');
                statusText.textContent = 'Connected';
                appendToConsole(`WebSocket connected to ${currentEnv} environment with session ${currentSession.id}`, 'success');
            };

            ws.onclose = () => {
                statusDot.classList.remove('connected');
                statusText.textContent = 'Disconnected';
                appendToConsole('WebSocket disconnected', 'error');
            };

            ws.onerror = () => {
                statusDot.classList.remove('connected');
                statusText.textContent = 'Error';
                appendToConsole('WebSocket error', 'error');
            };

            ws.onmessage = async (event) => {
                if (event.data instanceof Blob) {
                    appendToConsole('Received audio chunk', 'info');
                    // Convert blob to array buffer
                    const arrayBuffer = await event.data.arrayBuffer();
                    // Convert to Int16Array since we're receiving PCM 16-bit data
                    const pcmData = new Int16Array(arrayBuffer);
                    
                    // Convert PCM 16-bit to Float32 for Web Audio API
                    const floatData = new Float32Array(pcmData.length);
                    for (let i = 0; i < pcmData.length; i++) {
                        // Convert 16-bit integer to float32 (-1.0 to 1.0)
                        floatData[i] = pcmData[i] / 32768.0;
                    }
                    
                    // Resample the audio data to slow it down
                    const resampledData = new Float32Array(Math.floor(floatData.length * (PLAYBACK_SAMPLE_RATE / ORIGINAL_SAMPLE_RATE)));
                    for (let i = 0; i < resampledData.length; i++) {
                        const originalIndex = Math.floor(i * (ORIGINAL_SAMPLE_RATE / PLAYBACK_SAMPLE_RATE));
                        resampledData[i] = floatData[originalIndex];
                    }
                    
                    audioQueue.push(resampledData);
                    
                    // If not currently playing, start playing
                    if (!isPlaying) {
                        playNextChunk();
                    }
                } else {
                    // Handle text message
                    try {
                        const data = event.data;
                        appendToConsole(`Received: ${data}`, 'received');
                        
                        // Try to parse as JSON if it's a JSON string
                        try {
                            const jsonData = JSON.parse(data);
                            appendToConsole('Parsed JSON:', 'info');
                            appendToConsole(jsonData, 'received');
                        } catch (e) {
                            // If it's not JSON, we already logged the raw text
                        }
                    } catch (e) {
                        appendToConsole(`Error processing message: ${e.message}`, 'error');
                    }
                }
            };
        }

        async function playNextChunk() {
            if (audioQueue.length === 0) {
                isPlaying = false;
                responseIndicator.classList.remove('active');
                return;
            }

            isPlaying = true;
            responseIndicator.classList.add('active');

            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: PLAYBACK_SAMPLE_RATE
                });
            }

            const floatData = audioQueue.shift();
            const currentTime = audioContext.currentTime;
            
            // Create an audio buffer
            const audioBuffer = audioContext.createBuffer(1, floatData.length, PLAYBACK_SAMPLE_RATE);
            const channelData = audioBuffer.getChannelData(0);
            channelData.set(floatData);

            // Create and configure source
            const source = audioContext.createBufferSource();
            source.buffer = audioBuffer;
            source.playbackRate.value = PLAYBACK_RATE; // Set playback rate
            source.connect(audioContext.destination);
            
            // Calculate when this chunk should start playing
            const startTime = Math.max(currentTime, lastPlayTime);
            lastPlayTime = startTime + (audioBuffer.duration / PLAYBACK_RATE);
            
            // Schedule the next chunk
            source.onended = () => {
                if (audioQueue.length > 0) {
                    playNextChunk();
                } else {
                    isPlaying = false;
                    responseIndicator.classList.remove('active');
                }
            };

            try {
                source.start(startTime);
                appendToConsole('Playing audio chunk', 'info');
            } catch (error) {
                console.error('Error playing audio chunk:', error);
                appendToConsole('Error playing audio chunk: ' + error.message, 'error');
                // If there's an error, try the next chunk
                lastPlayTime = currentTime;
                playNextChunk();
            }
        }

        // Add event listener for microphone selection
        micSelect.addEventListener('change', async () => {
            if (currentAudioStream) {
                currentAudioStream.getTracks().forEach(track => track.stop());
            }
            if (micSelect.value) {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: { deviceId: { exact: micSelect.value } }
                    });
                    currentAudioStream = stream;
                    appendToConsole('Switched to microphone: ' + micSelect.selectedOptions[0].text, 'info');
                } catch (error) {
                    appendToConsole('Error switching microphone: ' + error.message, 'error');
                }
            }
        });

        // Function to enumerate audio devices
        async function setupAudioDevices() {
            try {
                const devices = await navigator.mediaDevices.enumerateDevices();
                const audioDevices = devices.filter(device => device.kind === 'audioinput');
                
                micSelect.innerHTML = '';
                let defaultDevice = null;

                audioDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Microphone ${index + 1}`;
                    micSelect.appendChild(option);
                    
                    // Check if this is the default device
                    if (device.deviceId === 'default') {
                        defaultDevice = device;
                    }
                });

                // If we found a default device, select it
                if (defaultDevice) {
                    micSelect.value = defaultDevice.deviceId;
                    // Trigger the change event to set up the stream
                    const event = new Event('change');
                    micSelect.dispatchEvent(event);
                } else if (audioDevices.length > 0) {
                    // If no default device, select the first one
                    micSelect.value = audioDevices[0].deviceId;
                    const event = new Event('change');
                    micSelect.dispatchEvent(event);
                }

                appendToConsole('Found ' + audioDevices.length + ' audio input devices', 'info');
            } catch (error) {
                appendToConsole('Error enumerating audio devices: ' + error.message, 'error');
            }
        }

        // Function to switch camera
        async function switchCamera(deviceId = null) {
            if (currentStream) {
                currentStream.getTracks().forEach(track => track.stop());
            }

            try {
                const constraints = {
                    video: deviceId ? 
                        { deviceId: { exact: deviceId } } : 
                        true
                };

                appendToConsole('Requesting camera with constraints: ' + JSON.stringify(constraints), 'info');
                
                const stream = await navigator.mediaDevices.getUserMedia(constraints);
                currentStream = stream;
                cameraFeed.srcObject = stream;
                
                // Ensure video is playing
                try {
                    await cameraFeed.play();
                    appendToConsole('Camera feed started successfully', 'success');
                } catch (playError) {
                    appendToConsole('Error playing video: ' + playError.message, 'error');
                }

                // Log active track settings
                const videoTrack = stream.getVideoTracks()[0];
                appendToConsole('Active video track settings: ' + JSON.stringify(videoTrack.getSettings()), 'info');
                
            } catch (error) {
                appendToConsole('Error accessing camera: ' + error.message, 'error');
            }
        }

        // Populate camera select with available devices
        async function setupCameras() {
            try {
                // First start with default camera
                await switchCamera();
                appendToConsole('Default camera initialized', 'success');

                // Then enumerate devices
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(device => device.kind === 'videoinput');
                
                appendToConsole('Found ' + videoDevices.length + ' video devices', 'info');
                
                cameraSelect.innerHTML = '';
                videoDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.text = device.label || `Camera ${index + 1}`;
                    cameraSelect.appendChild(option);
                    appendToConsole('Added camera option: ' + option.text, 'info');
                });

                // Select the current camera in the dropdown
                if (currentStream) {
                    const currentTrack = currentStream.getVideoTracks()[0];
                    if (currentTrack) {
                        const currentDevice = videoDevices.find(device => device.deviceId === currentTrack.getSettings().deviceId);
                        if (currentDevice) {
                            cameraSelect.value = currentDevice.deviceId;
                        }
                    }
                }

            } catch (error) {
                appendToConsole('Error setting up cameras: ' + error.message, 'error');
            }
        }

        // Handle camera selection
        cameraSelect.addEventListener('change', () => {
            if (cameraSelect.value) {
                appendToConsole('Switching to camera: ' + cameraSelect.selectedOptions[0].text, 'info');
                switchCamera(cameraSelect.value);
            }
        });

        // Update the capture button event listener to include session ID
        captureButton.addEventListener('click', async () => {
            try {
                if (!currentSession) {
                    appendToConsole('No active session. Please start a session first.', 'error');
                    return;
                }

                // Create a canvas element to capture the image
                const canvas = document.createElement('canvas');
                const context = canvas.getContext('2d');
                
                // Set canvas size to match video dimensions
                canvas.width = cameraFeed.videoWidth;
                canvas.height = cameraFeed.videoHeight;
                
                // Draw the current video frame to the canvas
                context.drawImage(cameraFeed, 0, 0, canvas.width, canvas.height);
                
                // Convert the canvas to a blob
                const blob = await new Promise(resolve => {
                    canvas.toBlob(resolve, 'image/jpeg', 0.95);
                });
                
                // Create form data and append the image
                const formData = new FormData();
                formData.append('image', blob, 'capture.jpg');
                
                appendToConsole('Sending image to server...', 'info');
                
                // Update description box with loading message
                const descriptionBox = document.getElementById('descriptionBox');
                descriptionBox.textContent = 'Waiting for message description...';
                
                // Send the image to the server with session ID
                const response = await fetch(`${envConfig[currentEnv].apiUrl}/conversations/image/${currentSession.id}`, {
                    method: 'POST',
                    body: formData
                });
                
                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }
                
                const data = await response.json();
                appendToConsole('Server response:', 'received');
                appendToConsole(data, 'received');
                
                // Update the description box with the transcription
                descriptionBox.textContent = data.transcription;
                
            } catch (error) {
                appendToConsole('Error capturing/sending image: ' + error.message, 'error');
                // Update description box with error message
                const descriptionBox = document.getElementById('descriptionBox');
                descriptionBox.textContent = 'Error: Failed to get image description';
            }
        });

        // Initialize camera system
        async function initializeCamera() {
            try {
                // Check if getUserMedia is supported
                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    throw new Error('Camera API not supported in this browser');
                }

                appendToConsole('Initializing camera system...', 'info');
                await setupCameras();

            } catch (error) {
                appendToConsole('Camera initialization error: ' + error.message, 'error');
            }
        }

        // Start camera initialization
        initializeCamera().catch(error => {
            appendToConsole('Fatal camera error: ' + error.message, 'error');
        });

        // Update session start functionality to connect WebSocket after session creation
        startSessionButton.addEventListener('click', async () => {
            try {
                startSessionButton.disabled = true;
                startSessionButton.textContent = 'Starting session...';
                
                const response = await fetch(`${envConfig[currentEnv].apiUrl}/conversations/sessions/`, {
                    method: 'POST'
                });

                if (!response.ok) {
                    throw new Error(`Failed to start session: ${response.status}`);
                }

                const data = await response.json();
                currentSession = data;
                appendToConsole('Session started successfully', 'success');
                appendToConsole(data, 'info');

                // Update UI
                startSessionButton.style.display = 'none';
                voiceButton.style.display = 'block';
                statusText.textContent = 'Connecting...';

                // Initialize WebSocket connection with session
                connect();

            } catch (error) {
                appendToConsole(`Error starting session: ${error.message}`, 'error');
                startSessionButton.disabled = false;
                startSessionButton.textContent = 'Start Session';
            }
        });

        // Update environment toggle to handle WebSocket properly
        envToggle.addEventListener('change', () => {
            currentEnv = envToggle.checked ? 'prod' : 'dev';
            // Close existing WebSocket if any
            if (ws) {
                ws.close();
                ws = null;
            }
            // Reset session when changing environments
            currentSession = null;
            localStorage.setItem('currentEnv', currentEnv);
            appendToConsole(`Switching to ${currentEnv} environment`, 'info');
            
            // Reset UI
            startSessionButton.style.display = 'block';
            startSessionButton.disabled = false;
            startSessionButton.textContent = 'Start Session';
            voiceButton.style.display = 'none';
            statusText.textContent = 'Ready';
        });

        // Initial setup
        setupAudioRecording();
    </script>
</body>
</html> 